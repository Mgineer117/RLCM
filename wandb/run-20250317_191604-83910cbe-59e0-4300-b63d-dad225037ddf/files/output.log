[36;1mSaving config:
[0m
{
    "K_epochs":	5,
    "action_dim":	2,
    "actor_dim":	[
        64,
        64
    ],
    "actor_lr":	0.0003,
    "algo_name":	"ppo",
    "critic_dim":	[
        128,
        128
    ],
    "critic_lr":	0.0005,
    "device":	"cuda:0",
    "entropy_scaler":	0.0005,
    "episode_len":	200,
    "eps":	0.2,
    "eval_episodes":	10,
    "eval_num":	10,
    "gae":	0.95,
    "gamma":	0.99,
    "gpu_idx":	0,
    "group":	"03-17_19-16-03.610693-be1b",
    "load_pretrained_model":	false,
    "log_intervals":	10,
    "logdir":	"log/train_log/03-17_19-16-03.610693-be1b",
    "minibatch_size":	512,
    "name":	"ppo-car-be1b-seed:1825",
    "num_minibatch":	10,
    "num_runs":	10,
    "project":	"Exp",
    "seed":	1825,
    "state_dim":	4,
    "target_kl":	0.02,
    "task":	"car",
    "timesteps":	3000000.0,
    "use_cuda":	true
}
PPO Training (Timesteps):  25%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž                                                                                                                                                                                                                                        | 762880/3000000.0 [03:02<08:54, 4181.74it/s]
Traceback (most recent call last):
  File "/home/minjae/research/RLCM/main.py", line 196, in <module>
    run(args, seed, unique_id, exp_time)
  File "/home/minjae/research/RLCM/main.py", line 176, in run
    trainer.train()
  File "/home/minjae/research/RLCM/trainer/online_trainer.py", line 82, in train
    loss_dict, ppo_timesteps, update_time = self.policy.learn(batch)
                                            ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/minjae/research/RLCM/policy/ppo.py", line 100, in learn
    advantages, returns = estimate_advantages(
                          ^^^^^^^^^^^^^^^^^^^^
  File "/home/minjae/research/RLCM/utils/rl.py", line 20, in estimate_advantages
    advantages[i] = deltas[i] + gamma * gae * prev_advantage * (1 - terminals[i])
                                                                ~~^~~~~~~~~~~~~~
  File "/home/minjae/miniconda3/envs/rlcm/lib/python3.12/site-packages/torch/_tensor.py", line 39, in wrapped
    return f(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^
  File "/home/minjae/miniconda3/envs/rlcm/lib/python3.12/site-packages/torch/_tensor.py", line 1073, in __rsub__
    return _C._VariableFunctions.rsub(self, other)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
KeyboardInterrupt
Exception ignored in atexit callback: <function _start_and_connect_service.<locals>.teardown_atexit at 0x7288875e32e0>
Traceback (most recent call last):
  File "/home/minjae/miniconda3/envs/rlcm/lib/python3.12/site-packages/wandb/sdk/lib/service_connection.py", line 94, in teardown_atexit
    conn.teardown(hooks.exit_code)
  File "/home/minjae/miniconda3/envs/rlcm/lib/python3.12/site-packages/wandb/sdk/lib/service_connection.py", line 226, in teardown
    self._router.join()
  File "/home/minjae/miniconda3/envs/rlcm/lib/python3.12/site-packages/wandb/sdk/interface/router.py", line 75, in join
    self._thread.join()
  File "/home/minjae/miniconda3/envs/rlcm/lib/python3.12/threading.py", line 1149, in join
    self._wait_for_tstate_lock()
  File "/home/minjae/miniconda3/envs/rlcm/lib/python3.12/threading.py", line 1169, in _wait_for_tstate_lock
    if lock.acquire(block, timeout):
       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
KeyboardInterrupt:
